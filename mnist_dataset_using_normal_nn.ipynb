{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist dataset using normal nn.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/gauravbansal98/mnist_dataset_using_normal_nn_1_06_2018/blob/master/mnist_dataset_using_normal_nn.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "X9OzX8amdduC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "11858224-4473-4b9c-b7d9-861fbe5f33ca"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\r\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L_UtNyfcdnbf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z2VTGOTTdtNG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jk1UZShwdwUC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "92bb489d-3425-4c3e-9a25-a700dde28c40"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"drive/mnist dataset using normal nn\", one_hot = True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-3a1fdb634e87>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting drive/mnist dataset using normal nn/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting drive/mnist dataset using normal nn/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting drive/mnist dataset using normal nn/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting drive/mnist dataset using normal nn/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JvhtZaxJdyOm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "84ce2fd3-8651-4558-91a7-4bce5c9bd8f3"
      },
      "cell_type": "code",
      "source": [
        "os.chdir('/content')\n",
        "!git clone https://github.com/mixuala/colab_utils\n",
        "import colab_utils.tboard\n",
        "ROOT = %pwd\n",
        "print(ROOT)\n",
        "LOG_DIR = os.path.join(ROOT, 'gaurav')\n",
        "print(LOG_DIR)\n",
        "colab_utils.tboard.launch_tensorboard( bin_dir=ROOT, log_dir=LOG_DIR )"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'colab_utils'...\n",
            "remote: Counting objects: 235, done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 235 (delta 6), reused 11 (delta 2), pack-reused 220\u001b[K\n",
            "Receiving objects: 100% (235/235), 74.58 KiB | 942.00 KiB/s, done.\n",
            "Resolving deltas: 100% (91/91), done.\n",
            "/content\n",
            "/content/gaurav\n",
            "calling wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip ...\n",
            "calling unzip ngrok-stable-linux-amd64.zip ...\n",
            "ngrok installed. path=/content/ngrok\n",
            "status: tensorboard=False, ngrok=False\n",
            "tensorboard url= http://e7879349.ngrok.io\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'http://e7879349.ngrok.io'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "7Awnt015eQHD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "no_of_layers = tf.constant(3, name = \"no_of_layers\")\n",
        "no_of_nodes_l1 = tf.constant(500, name = \"no_of_nodes_l1\")\n",
        "no_of_nodes_l2 = tf.constant(500, name = \"no_of_nodes_l2\")\n",
        "no_of_nodes_in_output_layer = tf.constant(10, name = \"no_of_nodes_in_output_layer\")\n",
        "batch_size = 128\n",
        "hm_epochs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zTG4vG1Recc0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = tf.placeholder(dtype = tf.float32, shape = [None, None], name = \"input_labels\")\n",
        "y = tf.placeholder(dtype = tf.float32, shape = [None, None], name = \"output_labels\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H1ML__G9eeVI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def neural_network_model(data):\n",
        "  \n",
        "  with tf.name_scope('neural_network'):\n",
        "  \n",
        "    hidden_layer1 = {\"weights_l1\" : tf.Variable(tf.random_normal([784, no_of_nodes_l1])),\n",
        "                     \"biases_l1\" : tf.Variable(tf.random_normal([1, no_of_nodes_l1]))}\n",
        "    hidden_layer2 = {\"weights_l2\" : tf.Variable(tf.random_normal([no_of_nodes_l1, no_of_nodes_l2])),\n",
        "                     \"biases_l2\" : tf.Variable(tf.random_normal([1, no_of_nodes_l2]))}\n",
        "    output_layer = {\"weights_ol\": tf.Variable(tf.random_normal([no_of_nodes_l2, no_of_nodes_in_output_layer])), \n",
        "                   \"biases_ol\": tf.Variable(tf.random_normal([1, no_of_nodes_in_output_layer]))}\n",
        "\n",
        "    z1 = tf.add(tf.matmul(data, hidden_layer1[\"weights_l1\"]), hidden_layer1[\"biases_l1\"])\n",
        "    a1 = tf.nn.relu(z1)\n",
        "    z2 = tf.add(tf.matmul(a1, hidden_layer2[\"weights_l2\"]), hidden_layer2[\"biases_l2\"])\n",
        "    a2 = tf.nn.relu(z2)\n",
        "  \n",
        "    output = tf.add(tf.matmul(a2, output_layer[\"weights_ol\"]), output_layer[\"biases_ol\"])\n",
        "\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QrHadftlegCS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_neural_network(x):\n",
        "\n",
        "  prediction = neural_network_model(x)\n",
        "  \n",
        "  with tf.name_scope('cost'):\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = prediction, labels = y))\n",
        "  summary = tf.summary.scalar('total_cost_of_batch', cost)\n",
        "  \n",
        "  with tf.name_scope('optimizer'):\n",
        "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
        "  \n",
        "  merged = tf.summary.merge_all()\n",
        "  \n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    train_writer = tf.summary.FileWriter('./gaurav',sess.graph)\n",
        "    k = 0\n",
        "    for i in range(hm_epochs):\n",
        "      epoch_loss = 0\n",
        "      for j in range(int(mnist.train.num_examples/batch_size)):\n",
        "        epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
        "        o, c, m = sess.run([optimizer, cost, merged], feed_dict = {x : epoch_x, y : epoch_y})\n",
        "        epoch_loss += c\n",
        "        train_writer.add_summary(m, k)\n",
        "        k += 1\n",
        "      print('epoch' , i, 'completed out of ', hm_epochs, 'loss ', epoch_loss)\n",
        "      \n",
        "      \n",
        "    correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
        "  \n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
        "  \n",
        "    print('Accuracy:',accuracy.eval({x:mnist.test.images, y:mnist.test.labels})) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xH5M7H9KemBI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "8bc81fe8-c6fe-4d7c-de99-15681a688294"
      },
      "cell_type": "code",
      "source": [
        "train_neural_network(x)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0 completed out of  10 loss  113543.1298904419\n",
            "epoch 1 completed out of  10 loss  29520.10003376007\n",
            "epoch 2 completed out of  10 loss  17828.107043981552\n",
            "epoch 3 completed out of  10 loss  11445.93170452118\n",
            "epoch 4 completed out of  10 loss  7563.912250354886\n",
            "epoch 5 completed out of  10 loss  5491.495393703692\n",
            "epoch 6 completed out of  10 loss  3786.446009475723\n",
            "epoch 7 completed out of  10 loss  2804.525605773786\n",
            "epoch 8 completed out of  10 loss  2097.379685644888\n",
            "epoch 9 completed out of  10 loss  1350.1484947205056\n",
            "Accuracy: 0.9503\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}